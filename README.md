

# üìã Pr√©sentation du Projet

Ce projet consiste en la transformation compl√®te de la cha√Æne de valeur des donn√©es d'AnyCompany. Nous avons mis en place une architecture robuste sur **Snowflake** et d√©velopp√© des solutions d'intelligence analytique avec **Python**. Le pipeline couvre l'ingestion (**Bronze**), le nettoyage (**Silver**), la structuration m√©tier (**Analytics**) et l'analyse pr√©dictive (**ML**).

# üë• L'√âquipe & R√©partition des T√¢ches

Ce travail est le fruit d'une collaboration structur√©e :

# üìÇ Structure du R√©pertoire

# ‚öôÔ∏è Installation & Pr√©requis

# üöÄ Workflow Technique

* **Ingestion & Nettoyage** : Passage des donn√©es brutes vers un √©tat "Ready-to-use" (SILVER).
* **Audit Automatis√©** : Contr√¥le d'int√©grit√© via une proc√©dure stock√©e Python dans Snowflake.
* **Analytics Engineering** : Cr√©ation de tables orient√©es m√©tier (CUSTOMER_360, SALES_HISTORY).
* **Intelligence Artificielle** : Extraction de la polarit√© des avis clients par traitement du langage naturel (NLP).

**Parties Communes** : Design de l'architecture Cloud, Gouvernance des donn√©es et Standardisation du sch√©ma SILVER.

**Technologies utilis√©es**

* *Base de donn√©es* : Snowflake (SQL)
* *Langage* : Python 3.x
* VS Code
* Git et GitHub
* Google Meet (pour des r√©unions)

**Librairies principales** - Pandas (Manipulation de donn√©es)

* Matplotlib & Seaborn (Visualisations avanc√©es)
* SQLAlchemy (Moteur de connexion)
* VADER (Analyse de sentiment lexicale)

# R√©partition des t√¢ches

# Florence : Data Preparation & Ingestion

## Activit√©s r√©alis√©es

1. Pr√©parer l‚Äôenvironnement Snowflake.
2. Cr√©er les diff√©rentes tables.
3. Charger les donn√©es.
4. V√©rifications et nettoyage des donn√©es (data cleaning) pour la fiabilit√© des donn√©es.

## Mode op√©ratoire :

1. Se connecter au compte Snowflake du projet :
[snowflake]
user = "FJCMMBAESG"
password = "Fjcmmbaesg020226!"
account = "BPHEGZS-EHB57068"
warehouse = "COMPUTE_WH"
database = "ANYCOMPANY_LAB"
schema = "SILVER"
2. Ouvrir le fichier SQL `load_data.sql` et lancer les codes bloc par bloc pour assurer une bonne ex√©cution des requ√™tes.
3. Ouvrir le fichier SQL `Clean_data.sql` et lancer les codes bloc par bloc pour assurer le bon chargement des donn√©es.

# Carole : Exploration des donn√©es et analyses business

## Activit√©s r√©alis√©es

1. Assurer la compr√©hension des jeux de donn√©es.
2. Faire des analyses exploratoires descriptives (Les 11 tables du sch√©ma SILVER).
3. Faire des analyses business transverses (fichier `campaign_performance.sql`, `promotion_impact.sql`, fichier `sales_trends.sql`).

## Mode op√©ratoire

Pour arriver √† voir l'ensemble des analyses, il faut :

1. Lancer les fichiers d√©di√©s √† la pr√©paration de donn√©es.
2. Lancer chaque fichier et les codes bloc par bloc pour visualiser chaque analyse s√©par√©ment.

# Marie Paule : Visualisations Streamlit

## Objectif

Les dashboards interactifs permettent :

* La visualisation des ventes dans le temps.
* L‚Äôanalyse par r√©gion et cat√©gorie.
* L‚Äôexploration des indicateurs supply chain.
* Le suivi de l‚Äôexp√©rience client.

## Mode op√©ratoire

1. Cloner le projet : `git clone https://github.com/florence93600/anycompany_food_beverage`
2. Installer les d√©pendances : `pip install streamlit snowflake-connector-python pandas plotly`
3. Configurer la connexion Snowflake :
* Cr√©er le fichier `.streamlit/secrets.toml` √† la racine du projet :
[snowflake]
user = "FJCMMBAESG"
password = "Fjcmmbaesg020226!"
account = "bphegzs-ehb57068"
warehouse = "COMPUTE_WH"
database = "ANYCOMPANY_LAB"
schema = "SILVER"
*P.S.* Ce fichier contient des identifiants : il ne doit jamais √™tre ajout√© sur GitHub.


4. Lancer les dashboards depuis le dossier `anycompany_food_beverage` :
* Dashboard Ventes & clients : `streamlit run streamlit/sales_dashboard.py`
* Dashboard Promotions, Stock & Logistique : `streamlit run streamlit/promotion_analysis.py`
* Dashboard Marketing ROI & Exp√©rience client : `streamlit run streamlit/marketing_roi.py`
*P.S.* Les dashboards s‚Äôouvrent sur : http://localhost:8501

# Missael : Data Products & Machine Learning

## Activit√©s r√©alis√©es

1. Enrichir et renommer 4 tables de Data Product (CUSTOMER_360, SALES_HISTORY, MARKETING_INITIATIVES, PRODUCT_SENTIMENT).
2. R√©aliser l'analyse de sentiment afin de d√©couvrir le ressenti ou l'exp√©rience globale des clients par rapport √† l'entreprise.

*P.S.* Ces activit√©s ont √©t√© r√©alis√©es sur VS Code en raison des contraintes rencontr√©es avec Snowflake, surtout lors de l'installation des packages Python n√©cessaires pour les analyses.

## Mode op√©ratoire :

1. Pr√©parer l'environnement de VS Code pour supporter les trois langages (Python, SQL et Markdown).
2. Installer les packages (`pandas`, `numpy`, `matplotlib`, `seaborn`, `vaderSentiment`, `sqlalchemy`, `snowflake-sqlalchemy`, `ipython-sql`).
3. S'assurer que les tables sont disponibles dans le sch√©ma SILVER sur Snowflake.
4. Connexion de VS Code aux donn√©es sources de Snowflake (tables dans SILVER).
5. Lancer le notebook (Run all).